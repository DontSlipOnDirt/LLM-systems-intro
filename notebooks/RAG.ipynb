{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9e15f55",
   "metadata": {},
   "source": [
    "This notebook provides example code for creating a basic RAG system using the LangChain framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3f8d887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82784eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Load a PDF and extract text\n",
    "pdf_path = \"data/div-class-title-relaxing-assumptions-improving-inference-integrating-machine-learning-and-the-linear-regression-div.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = loader.load()\n",
    "\n",
    "# Step 2: Split text into manageable chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "documents = text_splitter.split_documents(pages)\n",
    "\n",
    "# Step 3: Embed the documents using a HuggingFace model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 4: Store them in a Chroma vector store for retrieval\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"collection\",\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=\"./data/chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")\n",
    "vectorstore.add_documents(documents=documents)\n",
    "\n",
    "# Step 5: Load a local LLM from Hugging Face\n",
    "model_id = \"Gensyn/Qwen2.5-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "\n",
    "# Create a generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=300)\n",
    "\n",
    "# Wrap in LangChain's HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "# Step 6: Set up Retrieval-Augmented Generation (RAG)\n",
    "# Define a simple prompt\n",
    "prompt_template = \"\"\"You are an academic assistant. Use the following context to answer the user's question in a concise and helpful way.\n",
    "If you don't know the answer, say \"I don't know.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type=\"stuff\", \n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f28477d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Summarize PLCE\n",
      "Answer: The Partially Linear Causal Effect (PLCE) model is an extension of the partially linear model, incorporating exogenous interference, heteroskedasticity in the treatment assignment mechanism, and random effects. Under causal assumptions, it provides a causal estimate of the treatment on the outcome. The model includes equations for the treatment and outcome models, with specific conditions on the error terms. It differs from other econometric methods, such as the Generalized Method of Moments (GMM) or the Generalized Empirical Likelihood (GEL). The proposed model has been used to analyze racial threat distance turnout data. Overall, the PLCE model offers a flexible framework for causal analysis in situations involving complex interactions between variables.\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Ask questions based on PDF content\n",
    "query = \"Summarize PLCE\"\n",
    "print(\"Question:\", query)\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "# parse out actual answer from LLM response with regex\n",
    "raw_output = result[\"result\"]\n",
    "match = re.search(r\"Answer:\\s*(.*)\", raw_output, re.DOTALL)\n",
    "answer_text = match.group(1).strip()\n",
    "\n",
    "# Output result\n",
    "print(\"Answer:\", answer_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
